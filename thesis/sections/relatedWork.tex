\chapter{Related Work}
\label{chapter:relatedWork}

Table \ref{tab:feature_selection} provides an overview of FS methods with the focus on bioinformatics.
Ang et al. classify FS approaches according to their characteristics into the categories filter, wrapper, embedded, hybrid and ensemble\cite{ang2016supervised}.
Two studies from Bol\'{o}n-Canedo et al. compare the performance of different computational FS methods on synthetic and real world datasets\cite{bolon2014review}\cite{bolon2013review}.
Integrative FS is a specific field that gained attention in bioinformatics because it incorporates domain knowledge from external machine readable knowledge bases\cite{bellazzi2007towards}.
This approach allows better biological interpretation of the chosen features, further motivating the integration of external knowledge\cite{fang2014integrative}.

Qi and Tang integrate Gene Ontology (GO) into a FS approach\cite{qi2007integrating}.
First, discriminative scores are calculated for all genes by a filter approach.
Then, GO terms with all genes with a score higher than zero are extracted.
They assign each term a score which is the average of the gene scores.
Then they repeatedly select the gene with the highest score from the highest ranked GO term and recalculate the averages.
Compared to IG only they achieve better results on different cancer datasets.

Fang et al. developed an integrative FS method by combining IG with knowledge from KEGG and GO using association analysis\cite{fang2014integrative}.
Results on different cancer datasets show that the integrative approach performs well and selects relevant genes.
The usage of association rules leads to a high complexity in their approach.
An advantage of it is that information on the diseases of the dataset is necessary.
However, their FS is not only considering disease specific genes.

Quanz et al. developed an integrative FS approach that uses pathways as features\cite{quanz2008biological}. 
They use the global test to extract pathways related to the phenotypes of a dataset from the knowledge base KEGG\cite{kanehisa2000kegg}. 
The genes in each pathway are then transformed into one single feature by mean normalization or logistic regression.
An evaluation on an artificial and diabetes dataset showed improved performance over different computational approaches.
This method is a feature selection and transformation.
The usage of pathway extraction makes it complex.
Moreover, it was not tested in a multiclass and cancer problems.

Raghu et al. find maximally relevant and diverse feature sets with Preferential Diversity (PrefDiv) using an importance score that combines prior knowledge and the information inherent in the data\cite{raghu2017integrated}.
Then, graphical causal modeling is conducted by applying Mixed Graphical Models and the PC-Stable algorithm. 
In comparison to variance-based FS, using the top \emph{k} features according to the formulated importance score performs similarly or slightly better in a predictive modeling task while selecting causally relevant genes.
Their focus is to extract causal relationships in addition to addressing the high dimensionality of the data making their approach more complex.


\begin{table}[h!]
\centering
\hspace*{-1cm}\begin{tabular}{ | c | c | c | c | c | }
\hline
  type & functionality & characteristics & examples\\ \hline
  filter & uses only intrinsic & + independent of classifier & mRMR\cite{ding2005minimum}\\ 
  & data characteristics & + low complexity & ReliefF\cite{kononenko1994estimating}\\ 
  & & + good generalization & Information Gain (IG) \\\hline
  
  wrapper & uses learning algorithm & + detects feature dependencies & Genetic Algorithms (GA)\cite{ooi2003genetic}\\
  & to evaluate features & / interacts with classifier & Successive FS (SFS)\cite{sharma2012top}\\
  & & $-$ high complexity &\\
  & & $-$ risk of overfitting &\\\hline
  
  embedded & FS embedded & + detects feature dependencies & SVM-RFE\cite{guyon2002gene}\\ 
  & in learning algorithm & / interacts with classifier & Random Forest\cite{diaz2006gene}\\\hline
  
  hybrid & apply multiple ap- & / intermediate complexity & SVM-RFE + mRMR Filter\cite{mundra2010svm}\\ 
  & proaches sequentially & $-$ risk of slight overfitting & Multiple-filter-multiple- \\
  & &  & wrapper (MFMW) FS\cite{leung2010multiple}\\\hline
  
  ensemble & \makecell{uses aggregate of \\ group of feature sets} & \makecell{+ good for small \\ sample domains} & \makecell{Ensemble Gene Selection \\ By Grouping (EGSG)\cite{liu2010ensemble}}\\
  & & + less prone to overfitting & Multicriterion-fusion-based\\ 
  & & $-$ computationally expensive & Recursive Feature\\
  & & $-$ difficult to interpret & Elimination (MCF-RFE)\cite{yang2011robust}\\ \hline
  
  integrative & \makecell{combines external \\ knowledge and} & \makecell{+ interpretable for experts \\ $-$ domain-specific} & \makecell{GO / KEGG + FP-Growth  \\ Association Analysis}\cite{fang2014integrative}\\
  & \makecell{computational FS \\ ~} & \makecell{$-$ dependent on external \\  knowledge source} & \makecell{Preferential Diversity \\ + Graphical Causal Modeling}\cite{raghu2017integrated}\\ \hline
  
\end{tabular}
\caption{Overview on feature selection approaches focused on the bioinformatics domain}
\label{tab:feature_selection}
\end{table}  

\subsection{OLD RELATED WORK:}

Bolón-Canedo et al. analyze the performance of the eleven feature selection methods on eleven synthetic and two real world datasets when confronted with correlation, redundancy, nonlinearity, noise and small sample to feature ratios. They evaluate seven filter (Consistency-based, CFS, INTERACT, ReliefF, M\textsubscript{d}, Information Gain and mRMR), two wrapper (C4.5 and SVM) and two embedded (SVM-RFE and FS-Perceptron) feature selection methods. The evaluation is done using classification accuracy from four different classifiers (C4.5, Naive Bayes, IB1 and SVM) and their newly defined scoring measure which combines the absence of relevant features and the inclusion of irrelevant features. They conclude with recommending ReliefF generally for most scenarios. However, subset filters, Information Gain and SVM-RFE are advisable for scenarios with small sample to feature ratio like microarray experiments. Wrappers have the highest computational costs, performed worst on average and are not applicable in the aforementioned scenario. \cite{bolon2013review}

In a thorough survey of feature selection methods and microarray datasets, Bolón-Canedo et al. review various filter, wrapper, embedded, hybrid and ensemble feature selection methods. They describe different microarray datasets and the difficulties of dealing with this kind of data like small sample sizes and class imbalance. An experimental study is conducted with nine popular binary microarray datasets (two with a given training/test split and seven without) and seven different feature selection approaches (CFS, FCBF, INTERACT, Information Gain, ReliefF, mRMR and SVM-RFE). The feature selection methods can be distinguished as finding optimal subsets of features (CFS, FCBF, INTERACT) versus ranking them (Information Gain, ReliefF, mRMR and SVM-RFE). The chosen features were evaluated with the C4.5, Naive Bayes and SVM classifiers and accuracy, sensitivity and specificity were used as evaluation metrics. The results showed that subset filters worked better than rankers, that Naive Bayes and SVM outperform C4.5 and that Information Gain obtains similar performance to the other methods although being the only univariate filter used and therefore having lower complexity. \cite{Bolon-Canedo2014}

The embedded feature selection method SVM-RFE employs recursive feature elimination (RFE) utilizing the support vector machine (SVM) classifier to create a feature ranking. It works by repeatedly training the SVM and removing the feature with the lowest weight magnitude. A feature subset ranking is produced when multiple features are removed at once. The method was evaluated on two microarray datasets, leukemia and colon cancer. A better performance of this new technique compared to a correlation-based method called neighborhood analysis was shown both quantitatively as well as qualitatively by analyzing the biological relevance of the top selected genes for both datasets. Moreover, the usefulness of RFE compared to a naive feature ranking was demonstrated for small feature subsets over multiple classifiers. \cite{guyon2002gene}

The original, instance-based Relief feature selection algorithm works by randomly choosing one instance of a dataset, finding the nearest instance from the same and opposite class. The weights of all attributes are updated according to their difference of the selected instance to those neighbors. \cite{kira1992practical} This technique was extended to deal with noisy data by using the k nearest neighbors, to cope with incomplete data and to handle multi-class problems by averaging the  from near misses from each different class. This extension is called Relief-F and was it's ability to overcome the aforementioned problems was shown on different artificial as well as one real world dataset. \cite{kononenko1994estimating}

Integrative gene or feature selection approaches combine computational feature selection methods with existing biological background information obtained from knowledge bases. Fang et al. developed an integrative gene selection approach by combining information gain feature selection with biological knowledge. They build maximum frequent item sets using the FP-Growth association analysis algorithm on pathway annotations for genes from the Kyoto Encyclopedia of Genes and Genomes(KEGG) and Gene Ontology (GO). Those sets are ranked by their average information gain score and then from each set the gene with the highest information gain score is used for their feature ranking. The three integrative models (GO, KEGG and a GO-KEGG combination) were evaluated against another GO-based approach as well as the top 50 and 100 genes selected purely with information gain. The performance on colon, breast and lung cancer as well as leukemia datasets was measured using naive bayes, support vector machine and logistic regression classifiers. The results show that the integrative approach achieved a higher accuracy with a lower number of genes. Moreover, a qualitative analysis of the selected genes showed their connection to the respective diseases.\cite{fang2014integrative}

DUPLICATE TO SECTION ABOVE

Huey et al. developed an integrative gene selection approach that incorporates biological knowledge into the selection process. They first applied information gain to select features with discriminative power. Those genes were then associated with one another using annotations from either Gene Ontology or KEGG Pathways or both. This groups together genes with similar biological functions. An association analysis employing the FP-Growth algorithm mined frequent itemsets of genes co-occurring  often. Those sets were ranked by their average discriminative power as calculated from information gain. The final genes for classification were chosen by using the most discriminative gene from the top k frequent itemsets. This approach yielded better results than purely employing information gain on four different cancer datasets using Naive Bayes, Logistic Regression and Support Vector Machine classifiers. Moreover, they needed small amounts of selected genes to achieve this. This integration of biological knowledge furthermore allows a more meaningful interpretation of results. 
\cite{fang2014integrative}

Another integrative feature selection approach uses a two step workflow. In the first step, prior knowledge and the inherent information in gene expression data are combined by calculating an importance score which incorporates fold change of gene expression values and the gene-disease relevancy score from DisGeNET. Furthermore, a gene distance score is computed using chromosomal information, gene-disease relevancy scores, gene functional grouping data and the gene expression values. This is used to find maximally relevant and diverse feature sets with the Preferential Diversity (PrefDiv) framework. In the second step, graphical causal modeling is conducted by first preprocessing the data using Mixed Graphical Models and then applying the PC-Stable algorithm. The results show that the importance score and the PrefDiv feature ranking contain biologically relevant features compared with KEGG pathways. Moreover, in comparison to a variance-based feature selection method, using the top k features according to the formulated importance score performs similarly or slightly better in a predictive modeling task. There, the PrefDiv approach excels at finding relevant features.
\cite{raghu2017integrated}
