Bolón-Canedo et al. analyze the performance of the eleven feature selection methods on eleven synthetic and two real world datasets when confronted with correlation, redundancy, nonlinearity, noise and small sample to feature ratios. They evaluate seven filter (Consistency-based, CFS, INTERACT, ReliefF, M\textsubscript{d}, Information Gain and mRMR), two wrapper (C4.5 and SVM) and two embedded (SVM-RFE and FS-Perceptron) feature selection methods. The evaluation is done using classification accuracy from four different classifiers (C4.5, Naive Bayes, IB1 and SVM) and their newly defined scoring measure which combines the absence of relevant features and the inclusion of irrelevant features. They conclude with recommending ReliefF generally for most scenarios. However, subset filters, Information Gain and SVM-RFE are advisable for scenarios with small sample to feature ratio like microarray experiments. Wrappers have the highest computational costs, performed worst on average and are not applicable in the aforementioned scenario. \cite{bolon2013review}

In a thorough survey of feature selection methods and microarray datasets, Bolón-Canedo et al. review various filter, wrapper, embedded, hybrid and ensemble feature selection methods. They describe different microarray datasets and the difficulties of dealing with this kind of data like small sample sizes and class imbalance. An experimental study is conducted with nine popular binary microarray datasets (two with a given training/test split and seven without) and seven different feature selection approaches (CFS, FCBF, INTERACT, Information Gain, ReliefF, mRMR and SVM-RFE). The feature selection methods can be distinguished as finding optimal subsets of features (CFS, FCBF, INTERACT) versus ranking them (Information Gain, ReliefF, mRMR and SVM-RFE). The chosen features were evaluated with the C4.5, Naive Bayes and SVM classifiers and accuracy, sensitivity and specificity were used as evaluation metrics. The results showed that subset filters worked better than rankers, that Naive Bayes and SVM outperform C4.5 and that Information Gain obtains similar performance to the other methods although being the only univariate filter used and therefore having lower complexity. \cite{Bolon-Canedo2014}

The embedded feature selection method SVM-RFE employs recursive feature elimination (RFE) utilizing the support vector machine (SVM) classifier to create a feature ranking. It works by repeatedly training the SVM and removing the feature with the lowest weight magnitude. A feature subset ranking is produced when multiple features are removed at once. The method was evaluated on two microarray datasets, leukemia and colon cancer. A better performance of this new technique compared to a correlation-based method called neighborhood analysis was shown both quantitatively as well as qualitatively by analyzing the biological relevance of the top selected genes for both datasets. Moreover, the usefulness of RFE compared to a naive feature ranking was demonstrated for small feature subsets over multiple classifiers. \cite{Guyon2002}

The original, instance-based Relief feature selection algorithm works by randomly choosing one instance of a dataset, finding the nearest instance from the same and opposite class. The weights of all attributes are updated according to their difference of the selected instance to those neighbors. \cite{kira1992practical} This technique was extended to deal with noisy data by using the k nearest neighbors, to cope with incomplete data and to handle multi-class problems by averaging the  from near misses from each different class. This extension is called Relief-F and was it's ability to overcome the aforementioned problems was shown on different artificial as well as one real world dataset. \cite{Kononenko1994}

Integrative gene or feature selection approaches combine computational feature selection methods with existing biological background information obtained from knowledge bases. Fang et al. developed an integrative gene selection approach by combining information gain feature selection with biological knowledge. They build maximum frequent item sets using the FP-Growth association analysis algorithm on pathway annotations for genes from the Kyoto Encyclopedia of Genes and Genomes(KEGG) and Gene Ontology (GO). Those sets are ranked by their average information gain score and then from each set the gene with the highest information gain score is used for their feature ranking. The three integrative models (GO, KEGG and a GO-KEGG combination) were evaluated against another GO-based approach as well as the top 50 and 100 genes selected purely with information gain. The performance on colon, breast and lung cancer as well as leukemia datasets was measured using naive bayes, support vector machine and logistic regression classifiers. The results show that the integrative approach achieved a higher accuracy with a lower number of genes. Moreover, a qualitative analysis of the selected genes showed their connection to the respective diseases.\cite{fang2014integrative}

Another integrative feature selection approach uses a two step workflow. In the first step, prior knowledge and the inherent information in gene expression data are combined by calculating an importance score which incorporates fold change of gene expression values and the gene-disease relevancy score from DisGeNET. Furthermore, a gene distance score is computed using chromosomal information, gene-disease relevancy scores, gene functional grouping data and the gene expression values. This is used to find maximally relevant and diverse feature sets with the Preferential Diversity (PrefDiv) framework. In the second step, graphical causal modeling is conducted by first preprocessing the data using Mixed Graphical Models and then applying the PC-Stable algorithm. The results show that the importance score and the PrefDiv feature ranking contain biologically relevant features compared with KEGG pathways. Moreover, in comparison to a variance-based feature selection method, using the top k features according to the formulated importance score performs similarly or slightly better in a predictive modeling task. There, the PrefDiv approach excels at finding relevant features.
\cite{raghu2017integrated}